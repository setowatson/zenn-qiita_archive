---
title: LLMåŠ¹ç‡åŒ–ã«å¿…é ˆã®ã€ŒLangChain Expression Language (LCEL)ã€ã®ãƒ‡ãƒãƒƒã‚°ã«æ³¨æ„ã™ã‚‹ã€‚
tags:
  - debug
  - LangChain
  - LLM
  - LCEL
private: false
updated_at: '2025-03-09T11:46:38+09:00'
id: fa2c23f677bed6da0eea
organization_url_name: null
slide: false
ignorePublish: false
---
# LangChain Expression Language (LCEL)ã¨ã¯

LangChain Expression Language (LCEL)ã¨ã¯ã€**LangChainã®ä¸­ã§ä½¿ç”¨ã•ã‚Œã‚‹DSLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®è¨€èªï¼‰** ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚„LLMã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç›´æ„Ÿçš„ã«è¨˜è¿°ã§ãã‚‹ä»•çµ„ã¿ã§ã™ã€‚

ä¸»ã« **LLMã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®åŠ¹ç‡åŒ–** ã‚’ç›®çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

ä»Šå›ã¯ã“ã®LCELã®ç‰¹å¾´çš„ãªè¨˜è¿°æ–¹æ³•ã€ä½¿ã„æ–¹ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3780099/aafac8ca-8ec9-4411-ba7a-3385b0185d7f.png)


# 1. ç°¡å˜ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ§‹ç¯‰ã§ãã‚‹ï¼

LCELã® `|` è¨˜æ³•ã¯ã€LangChainã® `Runnable` ã‚’æ´»ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
Runnable ã®ç‰¹å¾´ã¨ã—ã¦ä»¥ä¸‹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚

- `invoke()`: åŒæœŸå®Ÿè¡Œ
- `ainvoke()`: éåŒæœŸå®Ÿè¡Œï¼ˆasyncio ã¨ã®è¦ªå’Œæ€§ãŒé«˜ã„ï¼‰
- `batch()`: ãƒãƒƒãƒå‡¦ç†ãŒå¯èƒ½ï¼ˆè¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€åº¦ã«å‡¦ç†ï¼‰



```ä¾‹1ï¼šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰.py
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt = PromptTemplate.from_template("Translate this into French: {input}")

# LLMãƒ¢ãƒ‡ãƒ«
llm = ChatOpenAI(model="gpt-4")

# LCELã® `|` è¨˜æ³•ã‚’ä½¿ã£ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
chain = prompt | llm

# å®Ÿè¡Œ
output = chain.invoke({"input": "Hello, how are you?"})
print(output)

```
ã“ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€`prompt | llm` ã®ã‚ˆã†ã« `|` ã‚’ä½¿ã†ã“ã¨ã§ã€
**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å‡¦ç† â†’ LLMã®å‘¼ã³å‡ºã—** ã‚’ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«ã¤ãªã’ã¦ã„ã¾ã™ã€‚


# 2. ä¸¦åˆ—å‡¦ç†ã§é€Ÿåº¦ã‚¢ãƒƒãƒ—ï¼


LCELã§ã¯ã€`ainvoke()` ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ **è¤‡æ•°ã®å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ** ã—ã€
å¤§è¦æ¨¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å‡¦ç†é€Ÿåº¦ã‚’æœ€é©åŒ–ã§ãã¾ã™ã€‚
ç‰¹ã« **LLMã®å‘¼ã³å‡ºã—ã‚’ä¸¦åˆ—åŒ–** ã™ã‚‹ã“ã¨ã§ã€å¿œç­”æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã§ãã‚‹ãŸã‚ã€
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã€ãƒ‡ãƒ¼ã‚¿è§£æãªã©ï¼‰ã§åŠ¹æœçš„ã§ã™ã€‚

```ä¾‹3ï¼šéåŒæœŸå‡¦ç†ã®ä¾‹.py
import asyncio
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt = PromptTemplate.from_template("Provide a recipe for {dish} in Japanese.")

# LLMãƒ¢ãƒ‡ãƒ«
llm = ChatOpenAI(model="gpt-4")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
chain = prompt | llm

# éåŒæœŸå®Ÿè¡Œ
async def async_task():
    result1 = chain.ainvoke({"dish": "ã‚«ãƒ¬ãƒ¼"})
    result2 = chain.ainvoke({"dish": "ãƒ©ãƒ¼ãƒ¡ãƒ³"})
    results = await asyncio.gather(result1, result2)  # ä¸¦åˆ—å‡¦ç†
    print(results)

asyncio.run(async_task())

```



# 3. çŸ¥ã£ã¦ãŠããŸã„ã€Œãƒ‡ãƒãƒƒã‚°ã€ã®ãƒã‚¤ãƒ³ãƒˆ

LCELã¯ã‚·ãƒ³ãƒ—ãƒ«ãªè¨˜æ³•ãªãŒã‚‰ã€ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚‚å¿…è¦ã«ãªã‚Šã¾ã™ã€‚
1ã§ãŠä¼ãˆã—ãŸ `|` è¨˜æ³•ã‚’æ´»ç”¨ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«è¨˜è¿°ã§ãã‚‹åé¢ã€ã‚¨ãƒ©ãƒ¼ã®ç‰¹å®šãŒé›£ã—ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

LCELã¯æŸ”è»Ÿã§å¼·åŠ›ã§ã™ãŒã€**ãƒ‡ãƒãƒƒã‚°ãŒã—ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãŠãã“ã¨ãŒé‡è¦** ã§ã™ã€‚



## ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ãƒ‡ãƒãƒƒã‚°æ–¹æ³•
LCELã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | è§£æ±ºç­– |
|--------|------|-------|
| `TypeError: 'str' object is not callable` | `Runnable` ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é–“é•ãˆã¦å‘¼ã³å‡ºã— | `invoke()` ã‚’é©åˆ‡ã«ä½¿ç”¨ã™ã‚‹ |
| `KeyError: 'input'` | `invoke()` ã«æ¸¡ã™è¾æ›¸ã®ã‚­ãƒ¼ãŒé–“é•ã£ã¦ã„ã‚‹ | `print(input_data.keys())` ã§ç¢ºèª |
| `asyncio.exceptions.TimeoutError` | LLMå‘¼ã³å‡ºã—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | `max_retries` ã‚’å¢—ã‚„ã™ãƒ»ãƒ¢ãƒ‡ãƒ«ã®è² è·ã‚’ç¢ºèª |
| `AttributeError: 'NoneType' object has no attribute '...` | ã©ã“ã‹ã§ `None` ãŒè¿”ã£ã¦ã„ã‚‹ | `print()` ã‚’æŒŸã‚“ã§ `None` ã®ç™ºç”Ÿç®‡æ‰€ã‚’ç‰¹å®š |

LCELã«é™ã‚‰ãªã„ã“ã¨ã‚‚å¤šã„ã§ã™ãŒã€ã€ã€ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã®åŸºæœ¬ã§ã™ã€‚

1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã‚€
2. `print()` or `RunnableLambda` ã‚’æŒŸã‚“ã§é€”ä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
3. éåŒæœŸå‡¦ç†ãªã‚‰ `try-except` ã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
4. è¾æ›¸ã®ã‚­ãƒ¼ã‚„ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„ã‹ç¢ºèª


ä»¥ä¸‹ã€å…·ä½“çš„ãªç¢ºèªãƒ»å¯¾å¿œæ–¹æ³•ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

##  3.1 `invoke()` ã§ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’ç¢ºèªã™ã‚‹ã€‚ 
LCELã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯è¤‡æ•°ã® `Runnable` ã‚’ã¤ãªã’ã‚‹ãŸã‚ã€é€”ä¸­ã®å‡¦ç†ãŒã©ã†ãªã£ã¦ã„ã‚‹ã®ã‹ç¢ºèªã—ã¥ã‚‰ããªã‚Šã¾ã™ã€‚  
ã¾ãšã¯ã€**å€‹ã€…ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’ç¢ºèª** ã—ã¦ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ã€‚


```ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’å€‹åˆ¥ã«ç¢ºèª.py
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«
prompt = PromptTemplate.from_template("What is {topic}?")
llm = ChatOpenAI(model="gpt-4")

# LCELãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
chain = prompt | llm

# ğŸ” é€”ä¸­ã®å‡¦ç†ã‚’ç¢ºèª
test_input = {"topic": "quantum computing"}
print("Step 1: Prompt Output ->", prompt.invoke(test_input))
print("Step 2: LLM Output ->", chain.invoke(test_input))
```

- **`invoke()` ã‚’å„ã‚¹ãƒ†ãƒƒãƒ—ã§å€‹åˆ¥ã«å®Ÿè¡Œ** ã—ã€ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
- LCELã® `|` ã‚’ä½¿ã†ã¨ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ãˆãªã„ã®ã§ã€**ä¸€éƒ¨ã®å‡¦ç†ã ã‘è©¦ã™** ã“ã¨ã§ãƒ‡ãƒãƒƒã‚°ãŒå®¹æ˜“ã«ãªã‚‹ã€‚



## 3.2 `RunnableLambda` ã§é€”ä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã€‚

LangChainã«ã¯ `RunnableLambda` ã¨ã„ã†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒã‚ã‚Šã€ä»»æ„ã®å‡¦ç†ã‚’æŒŸã‚ã¾ã™ã€‚  
**`RunnableLambda` ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ…‹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›** ã—ã€é€”ä¸­ã§å¤‰ãªãƒ‡ãƒ¼ã‚¿ã«ãªã£ã¦ã„ãªã„ã‹ç¢ºèªã§ãã¾ã™ã€‚


```ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’ãƒ­ã‚°å‡ºåŠ›.py
from langchain_core.runnables import RunnableLambda

def log_data(x):
    print("ğŸ” Debug:", x)
    return x

debug_chain = RunnableLambda(log_data) | chain

# å®Ÿè¡Œ
output = debug_chain.invoke({"topic": "AI ethics"})
print("Final Output:", output)
```

- `RunnableLambda` ã§ãƒ‡ãƒ¼ã‚¿ã‚’ **ãƒ­ã‚°ã«å‡ºåŠ›**
- é€”ä¸­ã®å‡¦ç†ãŒã©ã†ãªã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ãªãŒã‚‰ãƒ‡ãƒãƒƒã‚°ã§ãã‚‹
- **ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ…‹** ã‚’ãƒã‚§ãƒƒã‚¯ã—ã‚„ã™ããªã‚‹



## 3.3 `ainvoke()` ã®ãƒ‡ãƒãƒƒã‚°ï¼ˆéåŒæœŸå‡¦ç†ï¼‰

`ainvoke()` ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ `asyncio` ã®ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã«åŸ‹ã‚‚ã‚Œã¦ã—ã¾ã„ã€**ä½•ãŒåŸå› ãªã®ã‹åˆ†ã‹ã‚Šã¥ã‚‰ããªã‚‹** ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚  
ã“ã®å ´åˆã€`try-except` ã‚’ä½¿ã£ã¦ **ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è©³ç´°ã«å‡ºåŠ›** ã™ã‚‹ã¨ã€ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ããªã‚Šã¾ã™ã€‚


``` try-exceptã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°.py
import asyncio

async def debug_async():
    try:
        result1 = chain.ainvoke({"topic": "blockchain"})
        result2 = chain.ainvoke({"topic": "quantum physics"})
        results = await asyncio.gather(result1, result2)
        print("Results:", results)
    except Exception as e:
        print("ğŸš¨ Error occurred:", str(e))

asyncio.run(debug_async())
```

- `try-except` ã§ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒã—ã€åŸå› ã‚’ç‰¹å®š
- `asyncio.gather()` ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã€è¤‡æ•°ã®å‡¦ç†ã®ã©ã“ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‹ã‚’æŠŠæ¡ã—ã‚„ã™ãã™ã‚‹


## 3.4 `RunnableBranch` ã§æ¡ä»¶åˆ†å²ã‚’ãƒ‡ãƒãƒƒã‚°

LCELã§ã¯ `RunnableBranch` ã‚’ä½¿ã†ã¨ã€ç‰¹å®šã®æ¡ä»¶ã«å¿œã˜ã¦ç•°ãªã‚‹å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚  
ãŸã ã—ã€**æ¡ä»¶åˆ†å²ã®ãƒ­ã‚¸ãƒƒã‚¯ãŒé–“é•ã£ã¦ã„ã‚‹ã¨æ„å›³ã—ãªã„å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã‚‹** ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒãƒƒã‚°ãŒå¿…è¦ã§ã™ã€‚


```RunnableBranchã®ãƒ‡ãƒãƒƒã‚°.py
from langchain_core.runnables import RunnableBranch

def is_math_topic(input_data):
    return "math" in input_data["topic"].lower()

# æ¡ä»¶åˆ†å²
branch_chain = RunnableBranch(
    (is_math_topic, prompt | llm),
    (lambda x: True, RunnableLambda(lambda x: "Not a math topic"))
)

# ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
print(branch_chain.invoke({"topic": "math equations"}))  # LLMã«æ¸¡ã•ã‚Œã‚‹
print(branch_chain.invoke({"topic": "history"}))  # "Not a math topic" ãŒè¿”ã‚‹
```


- **`RunnableBranch` ã®æ¡ä»¶ãŒæ„å›³é€šã‚Šå‹•ä½œã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèª**
- **æ¡ä»¶é–¢æ•° (`is_math_topic`) ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç´°ã‹ããƒ†ã‚¹ãƒˆ** ã™ã‚‹
- **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ†å² (`lambda x: True`) ã‚’è¨­å®šã—ã¦ãŠãã¨ã€ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼ã§ãã‚‹**


